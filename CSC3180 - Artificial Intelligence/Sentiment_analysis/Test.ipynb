{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9482b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde35a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5) # CHANGE TO 2? / entropy classifier might also have to change\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae32f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0ddc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_1 = BertClassifier()\n",
    "model_1.load_state_dict(torch.load(\"models/model1.pth\"))\n",
    "model_1 = model_1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57ee752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment -  98.2173 %\n",
      "Negative Sentiment -  1.6782 %\n"
     ]
    }
   ],
   "source": [
    "# Use the Model\n",
    "\n",
    "text = \"\"\"Whenever watching the film, an eternally charming account of the world of toys, imagine that the story told is that of Pixar, the little company who proved, both through their own story and their film, the infinite possibility of imagination.\"\"\" # <-------- Input String\n",
    "\n",
    "# input filtering (todo)\n",
    "text = text.replace(\".\", \"\")\n",
    "text = text.replace(\",\", \"\")\n",
    "text = text.replace(\"-\", \"\")\n",
    "text = text.replace(\"`\", \"\")\n",
    "text = text.replace(\"'\", \"\")\n",
    "text = text.replace(\": \", \" \")\n",
    "text = text.replace(\"; \", \" \")\n",
    "text = text.replace(\"&amp;\", \"and\")\n",
    "text = text.replace(\"&quot;\", \"and\")\n",
    "text = text.replace(\"?\", \"\")\n",
    "text = text.replace(\"!\", \"\")\n",
    "text = text.replace(\"*\", \"\")\n",
    "text = text.replace('\"', \"\")\n",
    "text = text.replace('â€™', \"\")\n",
    "# -----------------------\n",
    "\n",
    "token = tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "result = softmax(model_1(token[\"input_ids\"].to(device), token[\"attention_mask\"].reshape([1,1,512]).to(device)))\n",
    "\n",
    "sentiment_negative = round(float(result[0][0]), 4)*100\n",
    "sentiment_positive = round(float(result[0][4]), 4)*100\n",
    "\n",
    "print(\"Positive Sentiment - \", round(float(result[0][-1])*100, 4), \"%\")\n",
    "print(\"Negative Sentiment - \", round(float(result[0][0])*100, 4), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b07e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
